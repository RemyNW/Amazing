{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../scripts'))\n",
    "from utils.load_env import PATH_DATA\n",
    "from feature_engineering import FeatureEngineering\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import polars as pl # Polars for memory-efficient data processing\n",
    "import gc\n",
    "\n",
    "pl.enable_string_cache() # Enable string cache to handle categorical comparisons\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def show_df(df, polar=True):\n",
    "    print(df.shape)\n",
    "    if polar:\n",
    "        display(pd.concat([df.head(2).to_pandas(), df.tail(1).to_pandas()]))\n",
    "    else:\n",
    "        display(pd.concat([df.head(2), df.tail(1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fa02b",
   "metadata": {},
   "source": [
    "## Description des données\n",
    "\n",
    "| Column | Description |\n",
    "|:-:|-|\n",
    "| event_time | Time when event happened at (in UTC). |\n",
    "| event_type | Type of event product_id ID of a product |\n",
    "| category_id | Product's category ID |\n",
    "| category_code | Product's category taxonomy (code name) if is was possible to make it. Usually present formeaningful categories and skipped for different kinds of accessories. Can be missing |\n",
    "| brand | Downcased string of brand name. Can be missing. |\n",
    "| price | Float price of the product. |\n",
    "| user_id | Permanent user ID. |\n",
    "| user_session | Temporary user's session ID. Same for each user's session. Is changed every time user come back to online store from a long pause. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fa0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\n",
    "    f\"{PATH_DATA}/2019-Oct.csv.gz\",\n",
    "    schema_overrides={\n",
    "        'user_id': pl.Categorical,\n",
    "        'product_id': pl.Categorical,\n",
    "        'category_id': pl.Categorical,\n",
    "        'price': pl.Float32,\n",
    "        'event_type': pl.Categorical,\n",
    "        'category_code': pl.Categorical,\n",
    "        'brand': pl.Categorical,\n",
    "        'user_session': pl.Categorical,\n",
    "        'event_time': pl.Utf8  # Load as string first, then parse\n",
    "    },\n",
    "    try_parse_dates=False,\n",
    "    infer_schema_length=1000,\n",
    "    n_rows=1000000\n",
    ").with_columns([\n",
    "    pl.col(\"event_time\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S %Z\", strict=False).alias(\"event_time\")\n",
    "])\n",
    "\n",
    "show_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b01593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load all months data\n",
    "def load_all_data():\n",
    "    \"\"\"Load all available months of data\"\"\"\n",
    "    months = ['2019-Oct', '2019-Nov', '2019-Dec', '2020-Jan', '2020-Feb', '2020-Mar', '2020-Apr']\n",
    "    dataframes = []\n",
    "    \n",
    "    for month in months:\n",
    "        try:\n",
    "            file_path = f\"{PATH_DATA}/{month}.csv.gz\"\n",
    "            print(f\"Loading {month}...\")\n",
    "            \n",
    "            df_month = pl.read_csv(\n",
    "                file_path,\n",
    "                schema_overrides={\n",
    "                    'user_id': pl.Categorical,\n",
    "                    'product_id': pl.Categorical,\n",
    "                    'category_id': pl.Categorical,\n",
    "                    'price': pl.Float32,\n",
    "                    'event_type': pl.Categorical,\n",
    "                    'category_code': pl.Categorical,\n",
    "                    'brand': pl.Categorical,\n",
    "                    'user_session': pl.Categorical,\n",
    "                    'event_time': pl.Utf8\n",
    "                },\n",
    "                try_parse_dates=False,\n",
    "                infer_schema_length=1000\n",
    "            ).with_columns([\n",
    "                pl.col(\"event_time\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S %Z\", strict=False).alias(\"event_time\"),\n",
    "                pl.lit(month).alias(\"month\")\n",
    "            ])\n",
    "            \n",
    "            print(f\"  - {month}: {df_month.shape[0]:,} rows\")\n",
    "            dataframes.append(df_month)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  - {month}: File not found or error - {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        combined_df = pl.concat(dataframes)\n",
    "        print(f\"\\nTotal combined data: {combined_df.shape[0]:,} rows, {combined_df.shape[1]} columns\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data files found, using sample data\")\n",
    "        return df.with_columns([pl.lit(\"2019-Oct\").alias(\"month\")])\n",
    "\n",
    "# Load all available data\n",
    "df_all = load_all_data()\n",
    "show_df(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d94e92",
   "metadata": {},
   "source": [
    "## Filtrage des utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8685ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count events per user efficiently with Polars\n",
    "user_counts_df = df_all.group_by(\"user_id\").len().sort(\"len\", descending=True)\n",
    "user_counts_series = user_counts_df.to_pandas().set_index('user_id')['len']\n",
    "\n",
    "# Calculate retention metrics\n",
    "event_distribution = user_counts_series.value_counts().sort_index()\n",
    "max_events = event_distribution.index.max()\n",
    "users_total = len(user_counts_series)\n",
    "users_with_at_least_n = event_distribution[::-1].cumsum()[::-1]\n",
    "n_events = np.arange(1, max_events + 1)\n",
    "users_retained = users_with_at_least_n.reindex(n_events, method='ffill').fillna(0)\n",
    "percent_users_lost = 1 - (users_retained / users_total)\n",
    "\n",
    "print(f\"Total users: {users_total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18327106",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 10\n",
    "\n",
    "n_display = np.arange(1, max_n + 1)\n",
    "percent_display = percent_users_lost.reindex(n_display, method='ffill').fillna(0) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_events, percent_users_lost*100, marker='o', linestyle='-')\n",
    "plt.xlim(0.95, max_n + 0.5)\n",
    "plt.xlabel(\"Minimum # of Events (n)\")\n",
    "plt.xticks(n_display)\n",
    "plt.ylabel(\"% of Users Lost\")\n",
    "plt.title(\"Users Lost vs. Minimum Number of Events\")\n",
    "plt.grid(True)\n",
    "\n",
    "for x, y in zip(n_display, percent_display):\n",
    "    plt.text(x, y + 1, f\"{y:.1f}%\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbecade",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_nb_events = 4  # 4 ou 5 semblent être les seuils les plus pertinents\n",
    "\n",
    "valid_users_df = (df_all\n",
    "    .group_by(\"user_id\")\n",
    "    .len()\n",
    "    .filter(pl.col(\"len\") >= min_nb_events)\n",
    "    .select(\"user_id\")\n",
    ")\n",
    "\n",
    "print(f\"Users with >= {min_nb_events} events: {valid_users_df.height:,}\")\n",
    "\n",
    "df_filtered = df_all.join(valid_users_df, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "print(f\"Original shape: {df_all.shape}\")\n",
    "print(f\"Filtered shape: {df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99002f73",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import feature_engineering\n",
    "importlib.reload(feature_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineering(df_filtered)\n",
    "\n",
    "engagement_features = fe.calculate_engagement_features(),\n",
    "purchase_features = fe.calculate_purchase_features(),\n",
    "product_preference_features = fe.calculate_product_preference_features(),\n",
    "temporal_features = fe.calculate_temporal_features(),\n",
    "rfm_features = fe.calculate_rfm_features(),\n",
    "behavioral_features = fe.calculate_behavioral_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs = [\n",
    "    engagement_features[0],\n",
    "    purchase_features[0],\n",
    "    product_preference_features[0],\n",
    "    temporal_features[0],\n",
    "    rfm_features[0],\n",
    "    behavioral_features\n",
    "]\n",
    "\n",
    "master_features = fe.all_users\n",
    "for feature_df in feature_dfs:\n",
    "    master_features = master_features.join(feature_df, on=\"user_id\", how=\"left\")\n",
    "            \n",
    "show_df(master_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f3295",
   "metadata": {},
   "source": [
    "## Analyse de la distribution des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features_distribution(df):\n",
    "    df_pandas = df.to_pandas()\n",
    "    features = df_pandas.drop(columns=\"user_id\")\n",
    "    n_features = len(features.columns)\n",
    "    # Create a large figure with subplots\n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(12, 3*n_features))\n",
    "    fig.suptitle('Feature Distribution Analysis', fontsize=16)\n",
    "\n",
    "    # Handle case where there's only one feature\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, feature in enumerate(features.columns):\n",
    "        # Get the data for this feature\n",
    "        feature_data = features[feature].dropna()\n",
    "        \n",
    "        # Boxplot (left subplot)\n",
    "        ax_box = axes[i, 0]\n",
    "        box_plot = ax_box.boxplot(feature_data, vert=True, patch_artist=True)\n",
    "        box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "        box_plot['boxes'][0].set_alpha(0.7)\n",
    "        ax_box.set_title(f'{feature} - Boxplot', fontsize=10)\n",
    "        ax_box.set_ylabel('Value')\n",
    "        ax_box.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = f'Mean: {feature_data.mean():.2f}\\nMedian: {feature_data.median():.2f}\\nStd: {feature_data.std():.2f}'\n",
    "        ax_box.text(0.02, 0.98, stats_text, transform=ax_box.transAxes, \n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                    fontsize=8)\n",
    "        \n",
    "        # Histogram (right subplot)\n",
    "        ax_hist = axes[i, 1]\n",
    "        n_bins = min(50, max(10, int(np.sqrt(len(feature_data)))))  # Adaptive number of bins\n",
    "        ax_hist.hist(feature_data, bins=n_bins, alpha=0.7, color='skyblue', edgecolor='black', linewidth=0.5)\n",
    "        ax_hist.set_title(f'{feature} - Histogram', fontsize=10)\n",
    "        ax_hist.set_xlabel('Value')\n",
    "        ax_hist.set_ylabel('Frequency')\n",
    "        ax_hist.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add distribution info\n",
    "        skewness = feature_data.skew()\n",
    "        kurtosis = feature_data.kurtosis()\n",
    "        dist_text = f'Skew: {skewness:.2f}\\nKurtosis: {kurtosis:.2f}\\nN: {len(feature_data):,}'\n",
    "        ax_hist.text(0.98, 0.98, dist_text, transform=ax_hist.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                    fontsize=8)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_df in feature_dfs:\n",
    "    visualize_features_distribution(feature_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
